# -*- coding: utf-8 -*-
"""submission.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1asnB00j57zCcFOoDbEE73fRRAzFAtDjW
"""

import pandas as pd
import numpy as np
import cv2
from keras.utils import to_categorical
import re
import spacy
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.applications.resnet_v2 import ResNet152V2
from keras.utils import plot_model
from sklearn.utils.class_weight import compute_class_weight
import matplotlib.pyplot as plt
from keras.layers import Dense, Concatenate, Input, Flatten, Embedding, CuDNNLSTM, Bidirectional, Dropout, LSTM
from keras.models import Model
from sklearn.dummy import DummyClassifier
import pickle
from keras.models import load_model

df = pd.read_csv("test_meme.csv", names=['image_name', 'Image_URL', 'OCR_extracted_text', 'Corrected_text']

#load test pics
X_test_pics = []
pic_ind = 0
error_pics = set()
for img in df['image_name']:
    try:
      im = cv2.imread(str(img))
      resized = cv2.resize(im, (256, 256), interpolation = cv2.INTER_AREA)
      X_test_pics.append(resized)
      print("Loaded pic no. " + str(pic_ind))
    except:
      print("Error loading pic no. " + str(pic_ind))
      error_pics.add(pic_ind)
    pic_ind += 1

#delete entries of pics that couldn't be loaded
df = df.drop([df.index[x] for x in error_pics])

#perform text preprocessing
X_test_text = df['Corrected_text'].apply(str).apply(lambda x: re.sub(r'[\S]+\.(net|com|org|info|edu|gov|uk|de|ca|jp|fr|au|us|ru|ch|it|nel|se|no|es|mil)[\S]*\s?','',x))
punctuation = '!"#$%&()*+-/:;<=>?@[\\]^_`{|}~'
X_test_text = X_test_text.apply(lambda x: ''.join(ch for ch in x if ch not in set(punctuation)))
X_test_text = X_test_text.str.lower()
X_test_text = X_test_text.str.replace("[0-9]", " ")
X_test_text = X_test_text.apply(lambda x:' '.join(x.split()))
nlp = spacy.load('en', disable=['parser', 'ner'])

def lemmatization(texts):
    output = []
    for i in texts:
        s = [token.lemma_ for token in nlp(i)]
        output.append(' '.join(s))
    return output
X_test_text = lemmatization(X_test_text)

#load tokenizer
infile = open("tokenizer",'rb')
tokenizer = pickle.load(infile)
infile.close()

X_test_text = tokenizer.texts_to_sequences(X_test_text)
X_test_text = pad_sequences(X_test_text, maxlen=169, padding='post')

#normalize images
X_test_pics = np.array(X_test_pics)/255

model_task1 = load_model('Task1_50_epochs.h5')
model_task2 = load_model('Task2_50_epochs.h5')
model_task3 = load_model('Task3_15_epochs.h5')

y_task1 = model_task1.predict([X_test_pics, X_test_text])
y_task1_preds = (y_task1 >= 0.5) + 0
y_task1_preds = y_task1_preds.replace(to_replace =[0, 1],  
                            value =["not_motivational", "motivational"])
y_task2 = model_task2.predict([X_test_pics, X_test_text])
y_task2_preds = np.argmax(y_task2, axis=1)
y_task2_preds = y_task2_preds.replace(to_replace =[0, 1, 2, 3, 4],  
                            value =["very_negative", "negative", "neutral", "positive", "very_positive"])
y_task3 = model_task3.predict([X_test_pics, X_test_text])
y_task3_preds = []
for i in range(len(y_task3)):
  if y_task3[i] < 0.5:
    y_task3_preds.append("not_offensive")
  elif y_task3[i] < 1.5:
    y_task3_preds.append("slight")
  elif y_task3[i] < 2.5:
    y_task3_preds.append("very_offensive")
  else:
    y_task3_preds.append("hateful_offensive")
y_task3_preds = np.array(y_task3_preds)

solution = pd.DataFrame({'Motivation': y_task1_preds, 'Overall_sentiment': y_task2_preds, 'Offense': y_task3_preds})
solution.to_csv("solution.csv", index=False)